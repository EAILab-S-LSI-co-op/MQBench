# configuration
extra_config:
  extra_qconfig_dict:
    w_observer: MSEObserver                     # custom weight observer
    a_observer: EMAMSEObserver                     # custom activation observer
    w_fakequantize: AdaRoundFakeQuantize        # custom weight fake quantize function
    a_fakequantize: QDropFakeQuantize           # custom activation fake quantize function
    w_qscheme:
      bit: 8                                    # custom bitwidth for weight,
      symmetry: False                           # custom whether quant is symmetric for weight,
      per_channel: True                         # custom whether quant is per-channel or per-tensor for weight,
      pot_scale: False                          # custom whether scale is power of two for weight.
      p: 2.4
    a_qscheme:
      bit: 8                                    # custom bitwidth for activation,
      symmetry: False                           # custom whether quant is symmetric for activation,
      per_channel: False                        # custom whether quant is per-channel or per-tensor for activation,
      pot_scale: False                          # custom whether scale is power of two for activation.
      p: 2.4

# advanced quantization configuration
quantize:
  backend: 'Academic'
  quantize_type: advanced_ptq                 # support naive_ptq or advanced_ptq
  cali_batchsize: 1
  reconstruction:
      pattern: block                          # 'layer' for Adaround or 'block' for BRECQ and QDROP
      scale_lr: 4.0e-5                        # learning rate for learning step size of activation
      warm_up: 0.2                            # 0.2 * max_count iters without regularization to floor or ceil
      weight: 0.01                            # loss weight for regularization item
      max_count: 20000                        # optimization iteration
      b_range: [20,2]                         # beta decaying range
      keep_gpu: True                          # calibration data restore in gpu or cpu
      round_mode: learned_hard_sigmoid        # ways to reconstruct the weight, currently only support learned_hard_sigmoid
      prob: 1.0                               # dropping probability of QDROP, 1.0 for Adaraound and BRECQ

model:
  path: /workspace/MQBench/example/ptq/models/cifar10_net.pth
  type: ConvNet                              # model name
data:
  path: /workspace/shared/data
  batch_size: 64
  num_workers: 4
  pin_memory: True
output:
  filename: academic_brecq_8bit